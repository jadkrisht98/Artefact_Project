{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94a3a343",
   "metadata": {},
   "source": [
    "#### Part 1 Data Preprocessing and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d0d601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns   \n",
    "import numpy as np  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85943077",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Jason mapping file\n",
    "\n",
    "with open('mappings.json_(DS_A-L2).json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "rows = [\n",
    "    {'tag': tag, 'offering': values[0], 'destination': values[1]}\n",
    "    for tag, values in data['tags_mapping'].items()\n",
    "]\n",
    "\n",
    "df_tags = pd.DataFrame(rows)\n",
    "df_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026bba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_tags['offering'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d31ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_tags['destination'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8730a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Customer feedback dataset\n",
    "\n",
    "feedback_df = pd.read_csv('dataset.csv_(DS_A-L2).csv')\n",
    "feedback_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a466585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "feedback_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee0ff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking into rows with NULL rating (contnent is not clear --> DROP them)\n",
    "feedback_df[feedback_df.ratings.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bf6de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop row will null rating\n",
    "feedback_df = feedback_df.dropna(subset=['ratings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3d9d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for duplicates\n",
    "feedback_df.drop('tags',axis=1).duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb8ab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_df[feedback_df.duplicated(keep=False)].sort_values(by='content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce84ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop duplicated rows\n",
    "feedback_df = feedback_df.drop_duplicates(subset=feedback_df.columns.difference(['tags']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232525c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Row with only emojis in content\n",
    "\n",
    "def is_only_emojis(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "        \"\\U00002700-\\U000027BF\"  # Dingbats\n",
    "        \"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    cleaned = emoji_pattern.sub('', str(text)).strip()\n",
    "    return cleaned == ''\n",
    "\n",
    "feedback_df[feedback_df['content'].apply(is_only_emojis)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479f2918",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping rows with only emojis\n",
    "feedback_df = feedback_df[~feedback_df['content'].apply(is_only_emojis)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fdd29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Invesitgating data types\n",
    "feedback_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d79632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert date column to date format \n",
    "feedback_df['date'] = pd.to_datetime(feedback_df['date'])\n",
    "\n",
    "#Convert a string representation of a list to an actual list\n",
    "feedback_df['tags'] = feedback_df['tags'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "feedback_df['ratings'] = feedback_df['ratings'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9079d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228c3bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Date Range\n",
    "date_range = feedback_df['date'].min().date(), feedback_df['date'].max().date()\n",
    "print(f\"Date Range: {date_range[0]} to {date_range[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2e87d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_df['title'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843c08ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploding 'tags' column into individual rows per list entry\n",
    "feedback_df_exploded = feedback_df.explode('tags')\n",
    "feedback_df_exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5182e461",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing 'tags' column with dictionary keys (value & sentiment) as new columns\n",
    "tags_df = feedback_df_exploded['tags'].apply(pd.Series)\n",
    "feedback_df_exploded = pd.concat([feedback_df_exploded.drop(columns=['tags']), tags_df], axis=1)\n",
    "feedback_df_exploded.rename(columns={'value': 'tag'}, inplace=True)\n",
    "\n",
    "#mapping tag with json file (df_tags) --Extracting Offering and Destination\n",
    "feedback_df_exploded = feedback_df_exploded.merge(df_tags, on='tag', how='left')\n",
    "\n",
    "feedback_df_exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c5bb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing 'ratings' column with keys (normalized & raw) as columns\n",
    "ratings_df = feedback_df_exploded['ratings'].apply(pd.Series)   \n",
    "feedback_df_exploded = pd.concat([feedback_df_exploded.drop(columns=['ratings']), ratings_df], axis=1)\n",
    "feedback_df_exploded.rename(columns={'normalized': 'normalized_rating'}, inplace=True)\n",
    "feedback_df_exploded.rename(columns={'raw': 'actual_rating'}, inplace=True)\n",
    "\n",
    "#rearranging columns\n",
    "feedback_df_exploded = feedback_df_exploded[['id', 'content', 'date', 'language', 'title', 'destination', 'offering','tag', 'normalized_rating', 'actual_rating','sentiment']]\n",
    "\n",
    "feedback_df_exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faadbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot encoding for 'offering' column\n",
    "final_df_encoded = pd.get_dummies(feedback_df_exploded, columns=['offering'],dtype=int)\n",
    "final_df_encoded['sentiment'] = final_df_encoded['sentiment'].fillna('missing')\n",
    "\n",
    "final_df = final_df_encoded.groupby(\n",
    "    ['id', 'content', 'date', 'language', 'title', 'normalized_rating', 'actual_rating', 'sentiment'],\n",
    "    as_index=False\n",
    ").agg({\n",
    "    'offering_Accommodation': 'sum',\n",
    "    'offering_Food & Beverage': 'sum',\n",
    "    'offering_Retail': 'sum',\n",
    "    'offering_Tourism Attractions/ Sites': 'sum',\n",
    "    'destination': list\n",
    "})\n",
    "\n",
    "final_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee546e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting offering_xxxx column into a 0/1 flag only\n",
    "cols = ['offering_Accommodation', 'offering_Food & Beverage', 'offering_Retail', 'offering_Tourism Attractions/ Sites']\n",
    "for col in cols:\n",
    "    final_df[col] = final_df[col].map(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cdfee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping more than one destination per content is not useful, so we will keep the most common destination for each content\n",
    "def get_most_common_destination(destinations):\n",
    "    destination_counts_dict = {}\n",
    "    for destination in destinations:\n",
    "        if destination in destination_counts_dict:\n",
    "            destination_counts_dict[destination] += 1\n",
    "        else:\n",
    "            destination_counts_dict[destination] = 1\n",
    "    return max(destination_counts_dict, key=destination_counts_dict.get)\n",
    "\n",
    "# Apply the function to the 'destination' column\n",
    "final_df['destination'] = final_df['destination'].apply(get_most_common_destination)\n",
    "\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b168a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding a sentiment to each row - METHOD 1  (Lexicon Based Approach)\n",
    "\n",
    "# For English Reviews\n",
    "from textblob import TextBlob\n",
    "def get_sentiment(text):\n",
    "    polarity = TextBlob(str(text)).sentiment.polarity\n",
    "    if polarity > 0.1:\n",
    "        return 'positive'\n",
    "    elif polarity < -0.1:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "    \n",
    "# For Arabic Reviews\n",
    "from camel_tools.sentiment import SentimentAnalyzer\n",
    "analyzer = SentimentAnalyzer.pretrained()\n",
    "\n",
    "def get_arabic_sentiment(text):\n",
    "    return analyzer.predict(text)\n",
    "\n",
    "\n",
    "# Apply sentiment analysis for English reviews\n",
    "final_df.loc[final_df['language'] == 'eng', 'sentiment'] = final_df.loc[final_df['language'] == 'eng', 'content'].apply(get_sentiment)\n",
    "\n",
    "# Apply sentiment analysis for Arabic reviews\n",
    "final_df.loc[final_df['language'] == 'ara', 'sentiment'] = final_df.loc[final_df['language'] == 'ara', 'content'].apply(get_arabic_sentiment)\n",
    "\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf21c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding a sentiment to each row - METHOD 2  (Task-Specific Large Language Model : XLM-RoBERTa pre-trained LLM model + Neural Network Classification model)\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load a multilingual sentiment analysis pipeline \n",
    "# XLM-RoBERTa converts each text into a embedding/vector then Neural Network layer classifies as Postive/Negative/Neutral\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n",
    "\n",
    "# Apply sentiment analysis to the 'content' column\n",
    "final_df['sentiment_LLM'] = final_df['content'].apply(lambda x: sentiment_pipeline(str(x))[0]['label'])\n",
    "\n",
    "# See the results\n",
    "print(final_df[['content', 'sentiment_LLM']].head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d048f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, sys\n",
    "print(\"python:\", sys.executable)\n",
    "print(\"torch:\", getattr(torch,\"__version__\",None))\n",
    "print(\"torch file:\", getattr(torch,\"__file__\",None))\n",
    "print(\"cuda available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e02788c",
   "metadata": {},
   "source": [
    "#### Part 2: Text Cleaning & NLP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62f7d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some content has both arabic (original) and english (translation) text both\n",
    "final_df[final_df.content.str.contains('(Translated by Google)')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d9abb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some content has both arabic (original) and english (translation) text both, so we keep english version only and make sure 'language' is set to 'eng'\n",
    "def clean_content_and_language(row):\n",
    "    content = row['content']\n",
    "    language = row['language']\n",
    "    if '(Translated by Google)' in content:\n",
    "        content = content.split('(Translated by Google)')[-1]\n",
    "        content = content.split('(Original)')[0]\n",
    "        language = 'eng'\n",
    "    return pd.Series([content, language])\n",
    "\n",
    "final_df[['content', 'language']] = final_df.apply(clean_content_and_language, axis=1)\n",
    "\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f9c655",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if language is correctly identified \n",
    "import re\n",
    "\n",
    "def detect_language_rule_based(text):\n",
    "    text = str(text)\n",
    "    has_english = re.search(r'[a-zA-Z]', text) is not None\n",
    "    has_arabic = re.search(r'[\\u0600-\\u06FF]', text) is not None\n",
    "    if has_english and has_arabic:\n",
    "        return 'mixed'\n",
    "    elif has_english:\n",
    "        return 'eng'\n",
    "    elif has_arabic:\n",
    "        return 'ara'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "final_df['language_2'] = final_df['content'].apply(detect_language_rule_based)\n",
    "\n",
    "mismatched_lang = final_df[final_df['language'] != final_df['language_2']]\n",
    "mismatched_lang[['id','content','language', 'language_2']].sort_values(by='language_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67cee97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are few rows that are misidentified\n",
    "mismatched_lang.groupby('language_2')['id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b64f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update the language column where language_2 is 'eng' or 'ara' only\n",
    "final_df.loc[final_df['language_2'].isin(['eng', 'ara']), 'language'] = final_df['language_2']\n",
    "final_df.drop('language_2', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b735874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For English reviews\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_english_text(text):\n",
    "    # Remove non-alphabetic characters (remove punctuation and numbers)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', str(text))\n",
    "\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenize and remove stopwords\n",
    "    words = [word for word in text.split() if word not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "    # # Stemming\n",
    "    # words = [stemmer.stem(word) for word in words]\n",
    "    # return ' '.join(words)\n",
    "\n",
    "\n",
    "\n",
    "final_df['clean_content'] = final_df.apply(lambda row: clean_english_text(row['content']) if row['language'] == 'eng' else None,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ebb689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Arabic reviews\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "from camel_tools.utils.dediac import dediac_ar\n",
    "from camel_tools.stem import ArabicLightStemmer\n",
    "from camel_tools.stopwords import stopwords_list\n",
    "\n",
    "arabic_stopwords = set(stopwords_list())\n",
    "stemmer_ar = ArabicLightStemmer()\n",
    "\n",
    "def clean_arabic_text(text):\n",
    "    # Remove diacritics\n",
    "    text = dediac_ar(str(text))\n",
    "    # Remove non-Arabic letters (keep spaces)\n",
    "    text = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)\n",
    "    # Tokenize\n",
    "    words = simple_word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in arabic_stopwords]\n",
    "    # lemmatization\n",
    "    words = [stemmer_ar.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "final_df['clean_content'] = final_df.apply(lambda row: clean_arabic_text(row['content']) if row['language'] == 'ara' else None,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b22aebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.loc[final_df['language'] == 'eng', ['content', 'clean_content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58c42ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Analysis (Common Keywords) - Frequency Based Approach (Term Frequency-Inverse Document Frequency: TF-IDF)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Compute TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(final_df['clean_content'].fillna(''))\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "dense = tfidf_matrix.todense()\n",
    "denselist = dense.tolist()\n",
    "df_tfidf = pd.DataFrame(denselist, columns=feature_names)\n",
    "common_keywords = df_tfidf.sum().nlargest(20)\n",
    "print(common_keywords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17edfcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Represent common words in a word cloud viz\n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(width=500, height=200, background_color='white').generate_from_frequencies(common_keywords)\n",
    "plt.figure(figsize=(7, 3))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d343040f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Analysis (Themes)  - METHOD 1 Topic Modeling with LDA (Classic Probabilistic Approach)\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Prepare the text data (replace None with empty string)\n",
    "texts = final_df['clean_content'].fillna('')\n",
    "\n",
    "# Vectorize the text\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Fit LDA model\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# Show n top words for each topic\n",
    "n=10\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic #{topic_idx+1}: \" + \", \".join([feature_names[i] for i in topic.argsort()[:-n:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9c3860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping dictionary after reviewing the keywords for each topic\n",
    "topic_to_theme = {\n",
    "    1: \"Restaurant & Service Experience\",\n",
    "    2: \"Family Outings & Parks\",\n",
    "    3: \"Hotels & Cleanliness\",\n",
    "    4: \"Parks & Value for Money\",\n",
    "    5: \"Religious Sites & Worship\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d89b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mapping topic with hihgets probability for each review\n",
    "\n",
    "doc_topic_dist = lda.transform(X)\n",
    "# Get the most probable topic \n",
    "most_probable_topic = np.argmax(doc_topic_dist, axis=1) +1\n",
    "final_df['lda_topic'] = most_probable_topic\n",
    "\n",
    "#Mapping the most probable topic to themes\n",
    "final_df['theme_LDA'] = final_df['lda_topic'].map(topic_to_theme)\n",
    "\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9153e6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Analysis (Themes)  - METHOD 2 Using Ensemble Model *BERTopic* (LLM + Clustering + Keyword Extraction)\n",
    "#BERTopic: Use BERT pre-trained LLM model for embedding + Clustering algorithm on embeddings via HDBSCAN + Keyword Extraction via TF-IDF for each cluster\n",
    "\n",
    "from bertopic import BERTopic\n",
    "\n",
    "# Prepare your review texts\n",
    "texts = final_df['content'].astype(str).tolist()\n",
    "\n",
    "# Create and fit BERTopic model (multilingual embeddings)\n",
    "topic_model = BERTopic(language=\"multilingual\")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(texts)\n",
    "final_df['theme_topic'] = topics\n",
    "\n",
    "# View topics and their keywords (Default is top 10 keywords per topic base on TF-IDF)\n",
    "topic_info = topic_model.get_topic_info()\n",
    "print(topic_info)\n",
    "\n",
    "# Show keywords for a specific topic\n",
    "print(topic_model.get_topic(0))  # Topic 0 keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd32aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping dictionary after reviewing the keywords for each topic\n",
    "topic_to_theme = {\n",
    "    0: \"Food & Restaurants\",\n",
    "    1: \"Religious Sites\",\n",
    "    2: \"Shopping\",\n",
    "    3: \"Accommodation\",\n",
    "    4: \"Transport\"\n",
    "    # ...add as needed\n",
    "}\n",
    "final_df['theme'] = final_df['theme_topic'].map(topic_to_theme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0587768e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADDING TOPIC REPRESENTATIONS LAYER\n",
    "#Use chat-gpt from open AI to fine tune topic representations instead of manually coming up with themes\n",
    "\n",
    "import openai\n",
    "client= openai.OpenAI(api_key='sk-...')\n",
    "representation_model=OpenAI(client, model='gpt-4o-mini',chat=True)\n",
    "topic_model=BERTopic(representation_model=representation_model, language=\"multilingual\") #Adding presentation model to BERTopic\n",
    "\n",
    "texts = final_df['content'].astype(str).tolist()\n",
    "topics, probs = topic_model.fit_transform(texts)\n",
    "final_df['theme_topic_chatgpt'] = topics\n",
    "\n",
    "topic_info = topic_model.get_topic_info()\n",
    "print(topic_info[['Topic', 'Name']])  # 'Name' column contains the theme\n",
    "\n",
    "# Create a mapping from topic number to theme name\n",
    "topic_to_theme = dict(zip(topic_info['Topic'], topic_info['Name']))\n",
    "\n",
    "# Assign theme to each review\n",
    "final_df['theme'] = final_df['theme_topic_chatgpt'].map(topic_to_theme)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad5038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47710011",
   "metadata": {},
   "source": [
    "#### Part 3: EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c44717",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of sentiments, offerings, destinations, and ratings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bbfbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment vs Offering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87d2cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment vs Destination\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b412f900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment vs Rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde6e4c3",
   "metadata": {},
   "source": [
    "#### Part4: Future Scope \n",
    "\n",
    "The data now is cleaned and labeled with themes --> ready to train an NLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd27691",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
